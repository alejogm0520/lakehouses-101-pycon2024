{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pockemon data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To connect to the Jupiter Server\n",
    "# In the docker attach screen, look for a message like this:\n",
    "#    http://127.0.0.1:8888/lab?token=xxxx\n",
    "# The \"xxxx\" is the password to be used when the Jupyter Kernel Connection ask for it...\n",
    "# Then select the \"Existing Jupiter Server\" option\n",
    "# Specify the URL: http://127.0.0.1:8888\n",
    "# Specify the password: xxxx\n",
    "# Select the desired Kernel from the list\n",
    "\n",
    "# References:\n",
    "# google: what is the default python jupiterlab server password\n",
    "# https://stackoverflow.com/questions/41117554/what-is-default-password-for-jupyter-created-on-googles-data-proc\n",
    "\n",
    "# https://github.com/jupyter/notebook/commit/7fa5d5a1be147e9c8e14f61a2f4b3c0db1e2c00b\n",
    "# For servers with token-authentication enabled, the URL in the above listing will include the token,\n",
    "# so you can copy and paste that URL into your browser to login."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Minio\n",
    "!sh /home/PyCon2024/Project/Scripts/1.init_minio.sh \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "import sys\n",
    "!{sys.executable} -m pip install pyspark\n",
    "!{sys.executable} -m pip install s3fs\n",
    "!{sys.executable} -m pip install minio\n",
    "!{sys.executable} -m pip install pyhive\n",
    "!{sys.executable} -m pip install trino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dotenv to load environment variables\n",
    "!{sys.executable} -m pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('minio.env')\n",
    "\n",
    "# Access the environment variables\n",
    "minio_access_key = os.getenv('MINIO_ACCESS_KEY')\n",
    "minio_secret_key = os.getenv('MINIO_SECRET_KEY')\n",
    "minio_endpoint = os.getenv('MINIO_ENDPOINT', \"http://minio:9000\")\n",
    "minio_bucket_name = os.getenv('MINIO_BUCKET_NAME', \"data-lakehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Minio Access Key:\", minio_access_key)\n",
    "print(\"Minio Secret Key:\", minio_secret_key)\n",
    "print(\"Minio Endpoint:\", minio_endpoint)\n",
    "print(\"Minio Bucket Name:\", minio_bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spark libraries\n",
    "import pyspark\n",
    "import pyspark.sql.functions as sqlF\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SLAlchemy and Pandas libraries\n",
    "from sqlalchemy.sql import text\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PokemonDataIngestion\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.1.0,org.apache.hadoop:hadoop-aws:3.3.3\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check spark configuration\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Build the data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('s3a://data-lakehouse/Raw/data/pokemon/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.show(truncate=False, n=10)\n",
    "df.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"name\", \"moves\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").save(\"s3a://data-lakehouse/bronze/pokemons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES from bronze\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_options = {\"path\": \"s3a://pruebas/bronze/new_table4\", \"overwriteSchema\": \"true\", \"mergeschema\": \"true\"}\n",
    "df.write.format(\"delta\").options(**additional_options).mode(\"overwrite\").saveAsTable(\"bronze.tabla4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = spark.read.format(\"delta\").table(\"bronze.pokemons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE HISTORY bronze.pokemons\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
